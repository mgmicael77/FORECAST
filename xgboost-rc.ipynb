{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This code is an implementation of a **time series forecasting model using XGBoost**, a popular gradient boosting algorithm. The model is trained on a dataset of unit sales data for a number of unique items and stores, and the goal is to predict future unit sales for each unique item-store combination.\n\nThe code begins by reading in the dataset and performing some preprocessing steps, including adding a unique ID column, dropping irrelevant columns, and one-hot encoding the category column. It then splits the data into training and test sets for each unique item-store combination and performs cross-validation on the training data using a TimeSeriesSplit.\n\nThe XGBoost model is then trained on the training data for each unique item-store combination, and the model is used to make predictions for the next day's unit sales using a sliding window approach. The predictions are stored in a dataframe along with the actual values, and the results are plotted for each unique item-store combination.\n\nOverall, the code implements a robust and flexible time series forecasting model that can be easily adapted to other datasets and forecasting problems.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom math import sqrt\nfrom sklearn.model_selection import TimeSeriesSplit\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\n\n# Define the path to 'total.csv'\n#total_csv_path = '/kaggle/input/total-final/total.csv'\ntotal_csv_path = '/kaggle/input/total-with-category/total_with_category.csv'\n\n# Load 'total.csv' and set index to 'date'\n#data = pd.read_csv(total_csv_path, parse_dates=['date'], index_col='date')\ndata = pd.read_csv(total_csv_path)\ndata = data[data['store'] == 548]\n\n# Add 'unique_id' column\ndata['unique_id'] = data['item_nbr'].astype(str) + '_' + data['store'].astype(str)\n\n# Drop 'item_nbr' and 'store' columns\ndata.drop(['item_nbr', 'store'], axis=1, inplace=True)\n\n# Select 10 random unique_ids and filter the data\nrandom_unique_ids = random.sample(list(data['unique_id'].unique()), 5)\ndata = data[data['unique_id'].isin(random_unique_ids)]\n\n#data = data.drop('index', axis=1)\n# Sort the data by 'unique_id' and 'date'\ndata.sort_values(by=['unique_id', 'date'], inplace=True)\n\n# Convert the date column to a datetime object and set it as the index\ndata['date'] = pd.to_datetime(data['date'])\ndata.set_index('date', inplace=True)\n                        \n# Assuming the data is already loaded into the DataFrame 'data'\ndata['category'] = data['category'].fillna('undefined')\n\n# One-hot encoding for the 'category' column if there are more categories\ndata = pd.get_dummies(data, columns=['category'])\n\ndata","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":38.687466,"end_time":"2023-05-02T11:01:20.349872","exception":false,"start_time":"2023-05-02T11:00:41.662406","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-10T13:29:40.292751Z","iopub.execute_input":"2023-05-10T13:29:40.293159Z","iopub.status.idle":"2023-05-10T13:30:01.181140Z","shell.execute_reply.started":"2023-05-10T13:29:40.293124Z","shell.execute_reply":"2023-05-10T13:30:01.180138Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"            unit_sales  bogo  circular_discount  circular  circedlp  coupon  \\\ndate                                                                          \n2021-01-23         1.0   0.0                0.0       0.0       0.0     0.0   \n2021-01-24         1.0   0.0                0.0       0.0       0.0     0.0   \n2021-01-25         1.0   0.0                0.0       0.0       0.0     0.0   \n2021-01-26         0.0   0.0                0.0       0.0       0.0     0.0   \n2021-01-27         0.0   0.0                0.0       0.0       0.0     0.0   \n...                ...   ...                ...       ...       ...     ...   \n2023-02-18        13.0   0.0                0.0       0.0       0.0     0.0   \n2023-02-19         9.0   0.0                0.0       0.0       0.0     0.0   \n2023-02-20         7.0   0.0                0.0       0.0       0.0     0.0   \n2023-02-21         3.0   0.0                0.0       0.0       0.0     0.0   \n2023-02-22         1.0   0.0                0.0       0.0       0.0     0.0   \n\n            discount_value  discount  fp_discount  front_page  fpedlp  gas  \\\ndate                                                                         \n2021-01-23             0.0       0.0          0.0         0.0     0.0  0.0   \n2021-01-24             0.0       0.0          0.0         0.0     0.0  0.0   \n2021-01-25             0.0       0.0          0.0         0.0     0.0  0.0   \n2021-01-26             0.0       0.0          0.0         0.0     0.0  0.0   \n2021-01-27             0.0       0.0          0.0         0.0     0.0  0.0   \n...                    ...       ...          ...         ...     ...  ...   \n2023-02-18             0.0       0.0          0.0         0.0     0.0  0.0   \n2023-02-19             0.0       0.0          0.0         0.0     0.0  0.0   \n2023-02-20             0.0       0.0          0.0         0.0     0.0  0.0   \n2023-02-21             0.0       0.0          0.0         0.0     0.0  0.0   \n2023-02-22             0.0       0.0          0.0         0.0     0.0  0.0   \n\n            nopromo      unique_id  category_DAIRY  category_GROCERY  \\\ndate                                                                   \n2021-01-23      1.0      11202_548               1                 0   \n2021-01-24      1.0      11202_548               1                 0   \n2021-01-25      1.0      11202_548               1                 0   \n2021-01-26      1.0      11202_548               1                 0   \n2021-01-27      1.0      11202_548               1                 0   \n...             ...            ...             ...               ...   \n2023-02-18      1.0  900004081_548               0                 0   \n2023-02-19      1.0  900004081_548               0                 0   \n2023-02-20      1.0  900004081_548               0                 0   \n2023-02-21      1.0  900004081_548               0                 0   \n2023-02-22      1.0  900004081_548               0                 0   \n\n            category_PRODUCE  \ndate                          \n2021-01-23                 0  \n2021-01-24                 0  \n2021-01-25                 0  \n2021-01-26                 0  \n2021-01-27                 0  \n...                      ...  \n2023-02-18                 1  \n2023-02-19                 1  \n2023-02-20                 1  \n2023-02-21                 1  \n2023-02-22                 1  \n\n[3805 rows x 17 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unit_sales</th>\n      <th>bogo</th>\n      <th>circular_discount</th>\n      <th>circular</th>\n      <th>circedlp</th>\n      <th>coupon</th>\n      <th>discount_value</th>\n      <th>discount</th>\n      <th>fp_discount</th>\n      <th>front_page</th>\n      <th>fpedlp</th>\n      <th>gas</th>\n      <th>nopromo</th>\n      <th>unique_id</th>\n      <th>category_DAIRY</th>\n      <th>category_GROCERY</th>\n      <th>category_PRODUCE</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2021-01-23</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>11202_548</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-01-24</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>11202_548</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-01-25</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>11202_548</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-01-26</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>11202_548</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-01-27</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>11202_548</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2023-02-18</th>\n      <td>13.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>900004081_548</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2023-02-19</th>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>900004081_548</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2023-02-20</th>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>900004081_548</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2023-02-21</th>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>900004081_548</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2023-02-22</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>900004081_548</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3805 rows Ã— 17 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Cross-validation** is a technique used to evaluate the performance of a machine learning model by training and testing it on different subsets of the data. In this case, the TimeSeriesSplit class is used to split the time series data into multiple folds to evaluate the model's performance over time.","metadata":{}},{"cell_type":"code","source":"train_ratio = 0.8\n\n# Create a dictionary to store train and test data for each unique_id\ntrain_test_data = {}\n\nfor i, unique_id in enumerate(random_unique_ids):\n    unique_id_data = data[data['unique_id'] == unique_id]\n    train_size = int(len(unique_id_data) * train_ratio)\n    train = unique_id_data.iloc[:train_size]\n    test = unique_id_data.iloc[train_size:]\n\n    # Perform cross-validation on the train set\n    train = train.sort_index()\n\n    tss = TimeSeriesSplit(n_splits=5, test_size=100, gap=1)\n\n    # Create a list to store train and test folds for each unique_id\n    train_test_folds = []\n    for train_idx, val_idx in tss.split(train):\n        train_fold = train.iloc[train_idx]\n        test_fold = train.iloc[val_idx]\n        train_test_folds.append((train_fold, test_fold))\n\n    # Store train and test data in the dictionary\n    train_test_data[unique_id] = {'train': train, 'test': test, 'folds': train_test_folds}\n","metadata":{"papermill":{"duration":1.855773,"end_time":"2023-05-02T11:01:22.753610","exception":false,"start_time":"2023-05-02T11:01:20.897837","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-10T13:30:59.064455Z","iopub.execute_input":"2023-05-10T13:30:59.064817Z","iopub.status.idle":"2023-05-10T13:30:59.092066Z","shell.execute_reply.started":"2023-05-10T13:30:59.064788Z","shell.execute_reply":"2023-05-10T13:30:59.091211Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{"papermill":{"duration":0.008925,"end_time":"2023-05-02T11:01:22.771754","exception":false,"start_time":"2023-05-02T11:01:22.762829","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Concatenate train sets from all unique_ids\nall_train_data = pd.concat([train_test_data[unique_id]['train'] for unique_id in random_unique_ids])\n\nall_train_data = all_train_data.sort_index()\n\n# Create a TimeSeriesSplit object\ntss = TimeSeriesSplit(n_splits=5, test_size=100, gap=1)\n\nfold = 0\npreds = []\nscores = []\nfor train_idx, val_idx in tss.split(all_train_data):\n    train_fold = all_train_data.iloc[train_idx]\n    val_fold = all_train_data.iloc[val_idx]\n\n    FEATURES = ['bogo', 'circular_discount', 'circular', 'circedlp', 'coupon', 'discount_value', 'discount', 'fp_discount', 'front_page', 'fpedlp', 'gas', 'nopromo']\n    TARGET = 'unit_sales'\n\n    X_train = train_fold[FEATURES]\n    y_train = train_fold[TARGET]\n\n    X_val = val_fold[FEATURES]\n    y_val = val_fold[TARGET]\n\n    reg = xgb.XGBRegressor(base_score=0.5, booster='gbtree',\n                           n_estimators=1000,\n                           early_stopping_rounds=50,\n                           objective='reg:squarederror',\n                           max_depth=3,\n                           learning_rate=0.01)\n    reg.fit(X_train, y_train,\n            eval_set=[(X_train, y_train), (X_val, y_val)],\n            verbose=100)\n\n    y_pred = reg.predict(X_val)\n    preds.append(y_pred)\n    score = np.sqrt(mean_squared_error(y_val, y_pred))\n    scores.append(score)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T13:31:18.374653Z","iopub.execute_input":"2023-05-10T13:31:18.375013Z","iopub.status.idle":"2023-05-10T13:31:19.304668Z","shell.execute_reply.started":"2023-05-10T13:31:18.374983Z","shell.execute_reply":"2023-05-10T13:31:19.303105Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 31\u001b[0m\n\u001b[1;32m     23\u001b[0m y_val \u001b[38;5;241m=\u001b[39m val_fold[TARGET]\n\u001b[1;32m     25\u001b[0m reg \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(base_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, booster\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgbtree\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     26\u001b[0m                        n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     27\u001b[0m                        early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     28\u001b[0m                        objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     29\u001b[0m                        max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     30\u001b[0m                        learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m reg\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m     36\u001b[0m preds\u001b[38;5;241m.\u001b[39mappend(y_pred)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:988\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity):\n\u001b[1;32m    987\u001b[0m     evals_result: TrainingCallback\u001b[38;5;241m.\u001b[39mEvalsLog \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 988\u001b[0m     train_dmatrix, evals \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:448\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_evaluation_matrices\u001b[39m(\n\u001b[1;32m    429\u001b[0m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    430\u001b[0m     X: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[1;32m    445\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m    446\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m    way.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m     train_dmatrix \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m     n_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py:908\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 908\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/core.py:743\u001b[0m, in \u001b[0;36mDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 743\u001b[0m handle, feature_names, feature_types \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_data_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnthread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m handle\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/data.py:970\u001b[0m, in \u001b[0;36mdispatch_data_backend\u001b[0;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_tuple(data, missing, threads, feature_names, feature_types)\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_df(data):\n\u001b[0;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_from_pandas_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_series(data):\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_pandas_series(\n\u001b[1;32m    974\u001b[0m         data, missing, threads, enable_categorical, feature_names, feature_types\n\u001b[1;32m    975\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/data.py:417\u001b[0m, in \u001b[0;36m_from_pandas_df\u001b[0;34m(data, enable_categorical, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_pandas_df\u001b[39m(\n\u001b[1;32m    410\u001b[0m     data: DataFrame,\n\u001b[1;32m    411\u001b[0m     enable_categorical: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[1;32m    416\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DispatchedDataBackendReturnType:\n\u001b[0;32m--> 417\u001b[0m     data, feature_names, feature_types \u001b[38;5;241m=\u001b[39m \u001b[43m_transform_pandas_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_numpy_array(data, missing, nthread, feature_names, feature_types)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/data.py:391\u001b[0m, in \u001b[0;36m_transform_pandas_df\u001b[0;34m(data, enable_categorical, feature_names, feature_types, meta, meta_type)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    380\u001b[0m     is_sparse,\n\u001b[1;32m    381\u001b[0m     is_categorical_dtype,\n\u001b[1;32m    382\u001b[0m )\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    385\u001b[0m     dtype\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m _pandas_dtype_mapper\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m is_sparse(dtype)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dtype \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtypes\n\u001b[1;32m    390\u001b[0m ):\n\u001b[0;32m--> 391\u001b[0m     \u001b[43m_invalid_dataframe_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m feature_names, feature_types \u001b[38;5;241m=\u001b[39m _pandas_feature_info(\n\u001b[1;32m    394\u001b[0m     data, meta, feature_names, feature_types, enable_categorical\n\u001b[1;32m    395\u001b[0m )\n\u001b[1;32m    397\u001b[0m transformed \u001b[38;5;241m=\u001b[39m _pandas_cat_null(data)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/data.py:283\u001b[0m, in \u001b[0;36m_invalid_dataframe_dtype\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    281\u001b[0m type_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame.dtypes for data must be int, float, bool or category.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_err\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_ENABLE_CAT_ERR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 283\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n","\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:unique_id: category"],"ename":"ValueError","evalue":"DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:unique_id: category","output_type":"error"}]},{"cell_type":"code","source":"print(f'Mean score across folds: {np.mean(scores):.4f}')\nprint(f'Fold scores: {scores}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEATURES = ['bogo', 'circular_discount', 'circular', 'circedlp', 'coupon', 'discount_value', 'discount', 'fp_discount', 'front_page', 'fpedlp', 'gas', 'nopromo']\nTARGET = 'unit_sales'\n\nX_all = all_train_data[FEATURES]\ny_all = all_train_data[TARGET]\n\nreg = xgb.XGBRegressor(base_score=0.5,\n                       booster='gbtree',\n                       n_estimators=500,\n                       objective='reg:squarederror',\n                       max_depth=3,\n                       learning_rate=0.01)\nreg.fit(X_all, y_all,\n        eval_set=[(X_all, y_all)],\n        verbose=100)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"from joblib import Parallel, delayed\nfrom tqdm import tqdm\n\n# Define a function for each iteration\ndef make_prediction(unique_id, train_test_data, reg, FEATURES, TARGET, window_size):\n    test_unique_id = train_test_data[unique_id]['test']\n    preds = []\n\n    # Iterate through the test set using the sliding window method\n    for i in range(0, len(test_unique_id) - window_size - horizon + 1):\n        # Select the window from the test set\n        window = test_unique_id.iloc[i:i+window_size]\n\n        # Prepare the features and target for the window\n        X_window = window[FEATURES]\n        y_window = window[TARGET]\n\n        # Fit the model on the window\n        reg.fit(X_window, y_window,\n                eval_set=[(X_window, y_window)],\n                verbose=100)\n\n        # Make a prediction for the next day\n        X_next_day = test_unique_id[FEATURES].iloc[i+window_size]\n        y_next_day = test_unique_id[TARGET].iloc[i+window_size]\n        y_pred = reg.predict(np.array(X_next_day).reshape(1, -1))\n\n        preds.append({\n            'unique_id': test_unique_id['unique_id'].iloc[i+window_size],\n            'date': test_unique_id.index[i+window_size],\n            'actuals': y_next_day,\n            'predicted': y_pred[0]\n        })\n\n    return preds\n\n# Use joblib and tqdm to parallelize and show the progress bar\npredictions = Parallel(n_jobs=-1)(\n    delayed(make_prediction)(unique_id, train_test_data, reg, FEATURES, TARGET, window_size)\n    for unique_id in tqdm(random_unique_ids)\n)\n\n# Prepare an empty DataFrame to store the predictions\npredictions_df = pd.DataFrame(columns=['unique_id', 'date', 'actuals', 'predicted'])\n\n# Concatenate the results\nfor preds in predictions:\n    predictions_df = pd.concat([predictions_df, pd.DataFrame(preds, columns=predictions_df.columns)], ignore_index=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the predictions DataFrame\npredictions_df['predicted'] = np.round(predictions_df['predicted'])\npredictions_df","metadata":{"papermill":{"duration":25.568515,"end_time":"2023-05-02T11:01:50.373936","exception":false,"start_time":"2023-05-02T11:01:24.805421","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Loop through each unique_id in the test set\nfor unique_id in random_unique_ids:\n    # Filter the data for the current unique_id\n    train_filtered = train_test_data[unique_id]['train']\n    test_filtered = train_test_data[unique_id]['test']\n    results_filtered = predictions_df[predictions_df['unique_id'] == unique_id]\n\n    # Extract the unit sales values for the current unique_id\n    Y_train_filtered = train_filtered[TARGET].values\n    Y_test_filtered = test_filtered[TARGET].values\n\n    # Get the predicted values directly from the 'results_filtered' DataFrame\n    Y_pred_filtered = results_filtered['predicted'].values\n\n    # Plot the training data, test data, and predicted values for the current unique_id\n    plt.figure(figsize=(15, 5))\n    plt.plot(train_filtered.index[-len(Y_train_filtered):], Y_train_filtered, label='Training Data')\n    plt.plot(test_filtered.index[:len(Y_test_filtered)], Y_test_filtered, label='Test Data')\n    plt.plot(results_filtered['date'][:len(Y_pred_filtered)], Y_pred_filtered, label='Predicted Values')\n    plt.title(f\"Time Series {unique_id}\")\n    plt.xlabel('Date')\n    plt.ylabel('Unit Sales')\n    plt.legend()\n    plt.show()","metadata":{"papermill":{"duration":0.592998,"end_time":"2023-05-02T11:01:51.036896","exception":false,"start_time":"2023-05-02T11:01:50.443898","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mean_absolute_scaled_error(actuals, predicted, seasonality=1):\n    mae = np.mean(np.abs(actuals - predicted))\n    naive_mae = np.mean(np.abs(actuals[seasonality:] - actuals[:-seasonality]))\n    return mae / naive_mae\n\nresults = []\n\n# Loop through each unique_id in the predictions_df\nfor unique_id in random_unique_ids:\n    unique_id_predictions = predictions_df[predictions_df['unique_id'] == unique_id]\n    actuals = unique_id_predictions['actuals'].values\n    predicted = unique_id_predictions['predicted'].values\n\n    # Calculate the error metrics\n    mae = mean_absolute_error(actuals, predicted)\n    rmse = np.sqrt(mean_squared_error(actuals, predicted))\n    mase = mean_absolute_scaled_error(actuals, predicted)\n\n    results.append({'unique_id': unique_id,\n                    'MAE': mae,\n                    'RMSE': rmse,\n                    'MASE': mase})\n\n# Save the results in a new DataFrame\nresults_df = pd.DataFrame(results)\n\nresults_df.to_csv('metrics_xgboost.csv', index=False)\n\nresults_df\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objs as go\n\n# Create a box plot for MAE\ntrace1 = go.Box(\n    y=results_df['MAE'],\n    name=\"MAE\",\n    boxpoints='outliers',\n    jitter=0.3,\n    pointpos=0,\n    boxmean=True\n)\n\n# Create a box plot for RMSE\ntrace2 = go.Box(\n    y=results_df['RMSE'],\n    name=\"RMSE\",\n    boxpoints='outliers',\n    jitter=0.3,\n    pointpos=0,\n    boxmean=True\n)\n\n# Create a box plot for MASE\ntrace3 = go.Box(\n    y=results_df['MASE'],\n    name=\"MASE\",\n    boxpoints='outliers',\n    jitter=0.3,\n    pointpos=0,\n    boxmean=True\n)\n\n# Create a layout for the box plots\nlayout = go.Layout(\n    title=\"Evaluation Metrics Box Plots\",\n    yaxis=dict(title=\"Value\"),\n    boxmode='group'\n)\n\n# Combine the traces and layout into a single figure\nfig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n\n# Plot the figure\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Interpretability","metadata":{}},{"cell_type":"markdown","source":"# Built-in Interpretability\n","metadata":{}},{"cell_type":"code","source":"xgb.plot_importance(reg)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for unique_id in models_final:\n    print(f\"Feature importances for unique_id: {unique_id}\")\n    xgb.plot_importance(models_final[unique_id])\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SHAP\n","metadata":{}},{"cell_type":"code","source":"import shap\n\n# print the JS visualization code to the notebook\nshap.initjs()\n\nfor unique_id in models_final:\n    print(f\"SHAP values and summary plot for unique_id: {unique_id}\")\n\n    # Create a TreeExplainer object\n    explainer = shap.TreeExplainer(models_final[unique_id])\n\n    # Calculate SHAP values\n    X_unique_id = train_test_data[unique_id]['train'][FEATURES]\n    shap_values = explainer.shap_values(X_unique_id)\n\n    # Plot SHAP summary plot\n    shap.summary_plot(shap_values, X_unique_id)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for unique_id in models_final:\n    print(f\"SHAP values and summary plot for unique_id: {unique_id}\")\n\n    # Create a TreeExplainer object\n    explainer = shap.TreeExplainer(models_final[unique_id])\n\n    # Calculate SHAP values for the validation data\n    X_unique_id = train_test_data[unique_id]['test'][FEATURES]\n    shap_values = explainer.shap_values(X_unique_id)\n\n    # Plot the SHAP summary plot\n    shap.summary_plot(shap_values, X_unique_id, plot_type=\"bar\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}