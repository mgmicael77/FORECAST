{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install pmdarima","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport joblib\nimport matplotlib as mpl\nimport pmdarima as pm\nimport random\n#import sktime\nimport sklearn\nfrom pmdarima import auto_arima\nfrom joblib import Parallel, delayed\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n#from sktime.performance_metrics.forecasting import mean_absolute_scaled_error\nfrom tqdm import tqdm\nimport pkg_resources\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else:\n            #df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n\n'''\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df\n'''\n\ndef import_data(file_path):\n    \"\"\"create a dataframe from the kaggle input and optimize its memory usage\"\"\"\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            if filename == file_path:\n                file = os.path.join(dirname, filename)\n                df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n                df = reduce_mem_usage(df)\n                return df\n    raise FileNotFoundError(\"File not found in Kaggle input directory.\")\n    \n    \ndef mean_absolute_scaled_error(y_true, y_pred, y_train):\n    \"\"\"\n    Calculate the Mean Absolute Scaled Error (MASE) of the predictions.\n\n    Args:\n    y_true (array-like): The true values of the target variable.\n    y_pred (array-like): The predicted values of the target variable.\n    y_train (array-like): The true values of the target variable for the training set.\n\n    Returns:\n    float: The MASE of the predictions.\n    \"\"\"\n    mae = mean_absolute_error(y_true, y_pred)\n    mae_naive = np.mean(np.abs(y_train[1:].to_numpy() - y_train[:-1].to_numpy()))\n    mase = mae / (mae_naive)\n    return mase\n\n\n\n# Load the data from the CSV file into a Pandas dataframe\ntotal = import_data(\"total.csv\")\n\n# Check the memory usage of the optimized dataframe\nprint(total.info(memory_usage='deep'))\n\n#total = total[(total['store'] == 548) | (total['store'] == 6100)]\ntotal = total[total['store'] == 548]\ntotal","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert 'date' column to datetime format\ntotal['date'] = pd.to_datetime(total['date'], format='%Y-%m-%d')\n\ntotal['unique_id'] = total['item_nbr'].astype(str) + '_' + total['store'].astype(str)\ntotal.drop(columns=['item_nbr', 'store'], inplace=True)\ntotal.reset_index(drop=True, inplace=True)\ntotal.drop(columns=['index'], inplace=True)\ntotal = total[['date', 'unique_id'] + [col for col in total.columns if col not in ['date', 'unique_id']]]\n\n#column with lagged values\ntotal['unit_sales_lag7'] = total.groupby('unique_id')['unit_sales'].shift(7)\ntotal.fillna(0, inplace=True)  # fill missing values with \n\nOptimization_Model = {}\n\n# get unique series IDs\nseries_ids = total['unique_id'].unique()\n\n###############################\n#series_ids = total['unique_id'].unique()\n\n# Select 10 random series IDs\n#random_series_ids = random.sample(list(series_ids), 10)\n#print(f\"Selected random series IDs: {random_series_ids}\")\n\n# Replace series_ids with the 10 random_series_ids\n#series_ids = random_series_ids\n###############################\n\ndef fit_sarimax(series_id):\n    \n    # select data for the current series\n    series = total[total['unique_id'] == series_id]\n    series.set_index('date', inplace=True)\n    \n    # Convert data types to float32 or float64\n    #series['unit_sales'] = series['unit_sales'].astype('float32')\n    series.loc[:, 'unit_sales'] = series['unit_sales'].astype('float32')\n\n    \n    # Prepare the exogenous variables (the additional columns)\n    exog = series.drop(columns=['unique_id', 'unit_sales']).astype('float32')\n    \n    # split the data into training and testing sets\n    train_size = int(len(series) * 0.8)\n    train, test = series[:train_size], series[train_size:]\n    \n    train_exog, test_exog = exog[:train_size], exog[train_size:]\n\n    # replace NaN values with the mean of the series\n    train = train.fillna(train.mean())\n\n    # replace infinite values with the max/min value of the series\n    train = train.replace([np.inf, -np.inf], [train.max(), train.min()])\n    \n    #print(\"Fitting SARIMAX model for series\", series_id)\n\n    # fit the SARIMAX model\n    #model_sarimax = auto_arima(train['unit_sales'], X=train_exog, seasonal=True, m=7, stepwise=True)\n    model_sarimax = auto_arima(train['unit_sales'], X=train_exog, seasonal=True, m=7, stepwise=True, trend='n')\n\n\n    # get model forecasts\n    sarimax_forecast, conf_int = model_sarimax.predict(n_periods=len(test), X=test_exog, return_conf_int=True)\n\n    # calculate evaluation metrics\n    sarimax_rmse = np.sqrt(mean_squared_error(test['unit_sales'], sarimax_forecast))\n    sarimax_mae = mean_absolute_error(test['unit_sales'], sarimax_forecast)\n    sarimax_r2 = r2_score(test['unit_sales'], sarimax_forecast)\n    sarimax_mase = mean_absolute_scaled_error(test['unit_sales'], sarimax_forecast, train['unit_sales'])\n    \n    # update the Optimization_Model dictionary\n    Optimization_Model[series_id] = {'order': model_sarimax.order,\n                                     'seasonal order': model_sarimax.seasonal_order,\n                                     'mae': sarimax_mae,\n                                     'rmse': sarimax_rmse,\n                                     'mase': sarimax_mase,\n                                     'r2': sarimax_r2}\n\n    #print(\"Fitted SARIMAX model for series\", series_id)\n    #print(Optimization_Model[series_id])\n    #print(\"\")\n\n    # return the evaluation metrics\n    return sarimax_rmse, sarimax_mae, sarimax_mase, sarimax_r2\n\n# fit SARIMAX models for all series in parallel\nresults = Parallel(n_jobs=-1, verbose=10)(delayed(fit_sarimax)(series_id) for series_id in tqdm(series_ids, desc='Fitting SARIMAX models'))\n\n# Create a dataframe of the results\nresults_df = pd.DataFrame(results, columns=['RMSE_SARIMAX', 'MAE_SARIMAX', 'MASE_SARIMAX', 'R2_SARIMAX'])\nresults_df.index = series_ids\n\n# Convert the Optimization_Model dictionary to a DataFrame\nmodel_params_df = pd.DataFrame.from_dict(Optimization_Model, orient='index')\n\n# Join the results_df DataFrame with the model_params_df DataFrame\nfinal_results_df = results_df.join(model_params_df)\n\n# Save the final_results_df to a CSV file\nfinal_results_df.to_csv('sarimax_results_and_params.csv', index=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}